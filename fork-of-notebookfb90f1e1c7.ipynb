{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T11:17:51.513251Z","iopub.execute_input":"2024-07-23T11:17:51.513582Z","iopub.status.idle":"2024-07-23T11:17:51.924419Z","shell.execute_reply.started":"2024-07-23T11:17:51.513552Z","shell.execute_reply":"2024-07-23T11:17:51.92317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\n\n# Reproducibility\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n# Load the CSV files\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_df = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\n# Extract labels and images from the DataFrame\ntrain_labels = train_df.pop('label').values\ntrain_images = train_df.values\n\n# Normalize the image data\ntrain_images = train_images / 255.0\n\n# Reshape the images to their original shape (assuming 28x28 pixels)\ntrain_images = train_images.reshape(-1, 28, 28, 1)\n\n# Convert to TensorFlow dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n\n# Split the dataset into training and validation sets\nval_size = int(0.2 * len(train_images))\nval_dataset = train_dataset.take(val_size)\ntrain_dataset = train_dataset.skip(val_size)\n\n# Batch and shuffle the datasets\nbatch_size = 64\ntrain_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    train_dataset\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    val_dataset\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:17:51.925914Z","iopub.execute_input":"2024-07-23T11:17:51.926434Z","iopub.status.idle":"2024-07-23T11:18:03.282876Z","shell.execute_reply.started":"2024-07-23T11:17:51.9264Z","shell.execute_reply":"2024-07-23T11:18:03.281642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport learntools.computer_vision.visiontools as visiontools\n\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:18:03.284264Z","iopub.execute_input":"2024-07-23T11:18:03.284671Z","iopub.status.idle":"2024-07-23T11:18:03.346604Z","shell.execute_reply.started":"2024-07-23T11:18:03.284632Z","shell.execute_reply":"2024-07-23T11:18:03.345425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    keras.Input(shape=(28, 28, 1)),  # Input layer\n    layers.RandomContrast(factor=0.5),  # Data augmentation\n    layers.RandomWidth(factor=0.15),    # Data augmentation\n    layers.RandomHeight(factor=0.15),   # Data augmentation\n    layers.RandomFlip(mode='horizontal'),  # Additional data augmentation\n    layers.Resizing(28, 28),  # Ensure fixed size after random width and height\n\n    layers.Conv2D(32, (3, 3), activation='relu'),  # First Conv layer\n    layers.BatchNormalization(),  # Batch normalization\n    layers.MaxPooling2D((2, 2)),  # First MaxPooling layer\n    layers.Dropout(0.25),  # Dropout to prevent overfitting\n\n    layers.Conv2D(64, (3, 3), activation='relu'),  # Second Conv layer\n    layers.BatchNormalization(),  # Batch normalization\n    layers.MaxPooling2D((2, 2)),  # Second MaxPooling layer\n    layers.Dropout(0.25),  # Dropout to prevent overfitting\n\n    layers.Conv2D(128, (3, 3), activation='relu'),  # Third Conv layer\n    layers.BatchNormalization(),  # Batch normalization\n    layers.MaxPooling2D((2, 2)),  # Third MaxPooling layer\n    layers.Dropout(0.25),  # Dropout to prevent overfitting\n\n    layers.Flatten(),  # Flatten layer\n\n    layers.Dense(128, activation='relu'),  # Dense layer\n    layers.BatchNormalization(),  # Batch normalization\n    layers.Dropout(0.5),  # Dropout to prevent overfitting\n\n    layers.Dense(10, activation='softmax')  # Output layer assuming 10 classes\n])\n\nmodel.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:18:03.349596Z","iopub.execute_input":"2024-07-23T11:18:03.350491Z","iopub.status.idle":"2024-07-23T11:18:03.569656Z","shell.execute_reply.started":"2024-07-23T11:18:03.350444Z","shell.execute_reply":"2024-07-23T11:18:03.568521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:18:03.571064Z","iopub.execute_input":"2024-07-23T11:18:03.57149Z","iopub.status.idle":"2024-07-23T11:18:03.58762Z","shell.execute_reply.started":"2024-07-23T11:18:03.57145Z","shell.execute_reply":"2024-07-23T11:18:03.586413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:18:26.43541Z","iopub.execute_input":"2024-07-23T11:18:26.435844Z","iopub.status.idle":"2024-07-23T11:27:19.967876Z","shell.execute_reply.started":"2024-07-23T11:18:26.435807Z","shell.execute_reply":"2024-07-23T11:27:19.966643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\n## Normalize the test images\ntest_images = test_df.values / 255.0\n\n# Reshape the test images to their original shape (28x28 pixels)\ntest_images = test_images.reshape(-1, 28, 28, 1)\n\n# Convert to TensorFlow dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_images)\n\n# Batch the test dataset\ntest_dataset = test_dataset.batch(batch_size)\n\n# Make predictions\npredictions = model.predict(test_dataset)\n\n# Convert predictions to class labels\npredicted_labels = np.argmax(predictions, axis=1)\n\n# Create a DataFrame with the submission data\nsubmission_df = pd.DataFrame({\n    'ImageId': np.arange(1, len(predicted_labels) + 1),\n    'Label': predicted_labels\n})\n\n# Save the DataFrame to a CSV file\nsubmission_file = 'submission.csv'\nsubmission_df.to_csv(submission_file, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:27:28.932651Z","iopub.execute_input":"2024-07-23T11:27:28.9331Z","iopub.status.idle":"2024-07-23T11:27:35.352508Z","shell.execute_reply.started":"2024-07-23T11:27:28.933064Z","shell.execute_reply":"2024-07-23T11:27:35.351448Z"},"trusted":true},"execution_count":null,"outputs":[]}]}